{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training on Nvidia 4070Ti super 16gb VRAM.\n",
    "# Import necessary libraries\n",
    "import functools\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from contextlib import nullcontext\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import torchvision.transforms.functional as TF\n",
    "import transformers\n",
    "import webdataset as wds\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from braceexpand import braceexpand\n",
    "from packaging import version\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict\n",
    "from torch.utils.data import default_collate\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, CLIPTextModel, PretrainedConfig\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    LCMScheduler,\n",
    "    StableDiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import resolve_interpolation_mode\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from webdataset.tariterators import (\n",
    "    base_plus_ext,\n",
    "    tar_file_expander,\n",
    "    url_opener,\n",
    "    valid_sample,\n",
    ")\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 77\n",
    "\n",
    "# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\n",
    "check_min_version(\"0.29.0.dev0\")\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    \"pretrained_teacher_model\": \"runwayml/stable-diffusion-v1-5\",\n",
    "    \"output_dir\": \"/home/node/academic/lcm/outputs\",\n",
    "    \"mixed_precision\": \"fp16\",\n",
    "    \"resolution\": 512,\n",
    "    \"lora_rank\": 128,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"loss_type\": \"huber\",\n",
    "    \"adam_weight_decay\": 0.0,\n",
    "    \"max_train_steps\": 1000,\n",
    "    \"max_train_samples\": 130000,\n",
    "    \"dataloader_num_workers\": 8,\n",
    "    \"train_shards_path_or_url\": \"/home/node/academic/lcmlora/00000.tar\",\n",
    "    \"validation_steps\": 200,\n",
    "    \"checkpointing_steps\": 200,\n",
    "    \"checkpoints_total_limit\": 10,\n",
    "    \"train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": ,\n",
    "    \"use_8bit_adam\": True,\n",
    "    \"seed\": 453645634,\n",
    "    \"lr_scheduler\": \"constant\",  \n",
    "    \"lr_warmup_steps\": 500,  \n",
    "    \"num_train_epochs\": 100,  \n",
    "    \"revision\": None,  \n",
    "    \"vae_encode_batch_size\": 32,  \n",
    "    \"w_min\": 5.0,  \n",
    "    \"w_max\": 15.0,  \n",
    "    \"enable_xformers_memory_efficient_attention\": False,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_kohya_state_dict(module, prefix: str, dtype: torch.dtype, adapter_name: str = \"default\"):\n",
    "    kohya_ss_state_dict = {}\n",
    "    for peft_key, weight in get_peft_model_state_dict(module, adapter_name=adapter_name).items():\n",
    "        kohya_key = peft_key.replace(\"base_model.model\", prefix)\n",
    "        kohya_key = kohya_key.replace(\"lora_A\", \"lora_down\")\n",
    "        kohya_key = kohya_key.replace(\"lora_B\", \"lora_up\")\n",
    "        kohya_key = kohya_key.replace(\".\", \"_\", kohya_key.count(\".\") - 2)\n",
    "        kohya_ss_state_dict[kohya_key] = weight.to(dtype)\n",
    "\n",
    "        # Set alpha parameter\n",
    "        if \"lora_down\" in kohya_key:\n",
    "            alpha_key = f'{kohya_key.split(\".\")[0]}.alpha'\n",
    "            kohya_ss_state_dict[alpha_key] = torch.tensor(module.peft_config[adapter_name].lora_alpha).to(dtype)\n",
    "\n",
    "    return kohya_ss_state_dict\n",
    "\n",
    "\n",
    "def filter_keys(key_set):\n",
    "    def _f(dictionary):\n",
    "        return {k: v for k, v in dictionary.items() if k in key_set}\n",
    "\n",
    "    return _f\n",
    "\n",
    "\n",
    "def group_by_keys_nothrow(data, keys=base_plus_ext, lcase=True, suffixes=None, handler=None):\n",
    "    current_sample = None\n",
    "    for filesample in data:\n",
    "        assert isinstance(filesample, dict)\n",
    "        fname, value = filesample[\"fname\"], filesample[\"data\"]\n",
    "        prefix, suffix = keys(fname)\n",
    "        if prefix is None:\n",
    "            continue\n",
    "        if lcase:\n",
    "            suffix = suffix.lower()\n",
    "        if current_sample is None or prefix != current_sample[\"__key__\"] or suffix in current_sample:\n",
    "            if valid_sample(current_sample):\n",
    "                yield current_sample\n",
    "            current_sample = {\"__key__\": prefix, \"__url__\": filesample[\"__url__\"]}\n",
    "        if suffixes is None or suffix in suffixes:\n",
    "            current_sample[suffix] = value\n",
    "    if valid_sample(current_sample):\n",
    "        yield current_sample\n",
    "\n",
    "\n",
    "def tarfile_to_samples_nothrow(src, handler=wds.warn_and_continue):\n",
    "    streams = url_opener(src, handler=handler)\n",
    "    files = tar_file_expander(streams, handler=handler)\n",
    "    samples = group_by_keys_nothrow(files, handler=handler)\n",
    "    return samples\n",
    "\n",
    "\n",
    "class WebdatasetFilter:\n",
    "    def __init__(self, min_size=1024, max_pwatermark=0.5):\n",
    "        self.min_size = min_size\n",
    "        self.max_pwatermark = max_pwatermark\n",
    "\n",
    "    def __call__(self, x):\n",
    "        try:\n",
    "            if \"json\" in x:\n",
    "                x_json = json.loads(x[\"json\"])\n",
    "                filter_size = (x_json.get(\"original_width\", 0.0) or 0.0) >= self.min_size and x_json.get(\n",
    "                    \"original_height\", 0\n",
    "                ) >= self.min_size\n",
    "                filter_watermark = (x_json.get(\"pwatermark\", 1.0) or 1.0) <= self.max_pwatermark\n",
    "                return filter_size and filter_watermark\n",
    "            else:\n",
    "                return False\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/node/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load scheduler, tokenizer, text encoder, VAE and teacher U-Net\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    config[\"pretrained_teacher_model\"], subfolder=\"scheduler\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config[\"pretrained_teacher_model\"], subfolder=\"tokenizer\", use_fast=False\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    config[\"pretrained_teacher_model\"], subfolder=\"text_encoder\"\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    config[\"pretrained_teacher_model\"], subfolder=\"vae\"\n",
    ")\n",
    "teacher_unet = UNet2DConditionModel.from_pretrained(\n",
    "    config[\"pretrained_teacher_model\"], subfolder=\"unet\"\n",
    ")\n",
    "\n",
    "# Freeze teacher models\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "teacher_unet.requires_grad_(False)\n",
    "\n",
    "# Create student U-Net\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    config[\"pretrained_teacher_model\"], subfolder=\"unet\"\n",
    ")\n",
    "unet.train()\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "class DDIMSolver:\n",
    "    def __init__(self, alpha_cumprods, timesteps=1000, ddim_timesteps=50):\n",
    "        step_ratio = timesteps // ddim_timesteps\n",
    "        self.ddim_timesteps = (np.arange(1, ddim_timesteps + 1) * step_ratio).round().astype(np.int64) - 1\n",
    "        self.ddim_alpha_cumprods = alpha_cumprods[self.ddim_timesteps]\n",
    "        self.ddim_alpha_cumprods_prev = np.asarray(\n",
    "            [alpha_cumprods[0]] + alpha_cumprods[self.ddim_timesteps[:-1]].tolist()\n",
    "        )\n",
    "        self.ddim_timesteps = torch.from_numpy(self.ddim_timesteps).long()\n",
    "        self.ddim_alpha_cumprods = torch.from_numpy(self.ddim_alpha_cumprods)\n",
    "        self.ddim_alpha_cumprods_prev = torch.from_numpy(self.ddim_alpha_cumprods_prev)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.ddim_timesteps = self.ddim_timesteps.to(device)\n",
    "        self.ddim_alpha_cumprods = self.ddim_alpha_cumprods.to(device)\n",
    "        self.ddim_alpha_cumprods_prev = self.ddim_alpha_cumprods_prev.to(device)\n",
    "        return self\n",
    "\n",
    "    def ddim_step(self, pred_x0, pred_noise, timestep_index):\n",
    "        alpha_cumprod_prev = extract_into_tensor(self.ddim_alpha_cumprods_prev, timestep_index, pred_x0.shape)\n",
    "        dir_xt = (1.0 - alpha_cumprod_prev).sqrt() * pred_noise\n",
    "        x_prev = alpha_cumprod_prev.sqrt() * pred_x0 + dir_xt\n",
    "        return x_prev\n",
    "# Load noise schedules and initialize DDIM solver\n",
    "alpha_schedule = torch.sqrt(noise_scheduler.alphas_cumprod)\n",
    "sigma_schedule = torch.sqrt(1 - noise_scheduler.alphas_cumprod)\n",
    "solver = DDIMSolver(\n",
    "    noise_scheduler.alphas_cumprod.numpy(),\n",
    "    timesteps=noise_scheduler.config.num_train_timesteps,\n",
    "    ddim_timesteps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDText2ImageDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_shards_path_or_url: Union[str, List[str]],\n",
    "        num_train_examples: int,\n",
    "        per_gpu_batch_size: int,\n",
    "        global_batch_size: int,\n",
    "        num_workers: int,\n",
    "        resolution: int = 512,\n",
    "        interpolation_type: str = \"bilinear\",\n",
    "        shuffle_buffer_size: int = 1000,\n",
    "        pin_memory: bool = False,\n",
    "        persistent_workers: bool = False,\n",
    "    ):\n",
    "        if not isinstance(train_shards_path_or_url, str):\n",
    "            train_shards_path_or_url = [list(braceexpand(urls)) for urls in train_shards_path_or_url]\n",
    "            train_shards_path_or_url = list(itertools.chain.from_iterable(train_shards_path_or_url))\n",
    "\n",
    "        interpolation_mode = resolve_interpolation_mode(interpolation_type)\n",
    "\n",
    "        def transform(example):\n",
    "            image = example[\"image\"]\n",
    "            image = TF.resize(image, resolution, interpolation=interpolation_mode)\n",
    "\n",
    "            c_top, c_left, _, _ = transforms.RandomCrop.get_params(image, output_size=(resolution, resolution))\n",
    "            image = TF.crop(image, c_top, c_left, resolution, resolution)\n",
    "            image = TF.to_tensor(image)\n",
    "            image = TF.normalize(image, [0.5], [0.5])\n",
    "\n",
    "            example[\"image\"] = image\n",
    "            return example\n",
    "\n",
    "        processing_pipeline = [\n",
    "            wds.decode(\"pil\", handler=wds.ignore_and_continue),\n",
    "            wds.rename(image=\"jpg;png;jpeg;webp\", text=\"text;txt;caption\", handler=wds.warn_and_continue),\n",
    "            wds.map(filter_keys({\"image\", \"text\"})),\n",
    "            wds.map(transform),\n",
    "            wds.to_tuple(\"image\", \"text\"),\n",
    "        ]\n",
    "\n",
    "        pipeline = [\n",
    "            wds.ResampledShards(train_shards_path_or_url),\n",
    "            tarfile_to_samples_nothrow,\n",
    "            wds.shuffle(shuffle_buffer_size),\n",
    "            *processing_pipeline,\n",
    "            wds.batched(per_gpu_batch_size, partial=False, collation_fn=default_collate),\n",
    "        ]\n",
    "\n",
    "        num_worker_batches = math.ceil(num_train_examples / (global_batch_size * num_workers))\n",
    "        num_batches = num_worker_batches * num_workers\n",
    "        num_samples = num_batches * global_batch_size\n",
    "\n",
    "        self._train_dataset = wds.DataPipeline(*pipeline).with_epoch(num_worker_batches)\n",
    "        self._train_dataloader = wds.WebLoader(\n",
    "            self._train_dataset,\n",
    "            batch_size=None,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "        )\n",
    "        self._train_dataloader.num_batches = num_batches\n",
    "        self._train_dataloader.num_samples = num_samples\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self._train_dataset\n",
    "\n",
    "    @property\n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_validation(vae, unet, config, accelerator, weight_dtype, step):\n",
    "    logger.info(\"Running validation...\")\n",
    "    if torch.backends.mps.is_available():\n",
    "        autocast_ctx = nullcontext()\n",
    "    else:\n",
    "        autocast_ctx = torch.autocast(accelerator.device.type, dtype=weight_dtype)\n",
    "\n",
    "    unet = accelerator.unwrap_model(unet)\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        config[\"pretrained_teacher_model\"],\n",
    "        vae=vae,\n",
    "        scheduler=LCMScheduler.from_pretrained(config[\"pretrained_teacher_model\"], subfolder=\"scheduler\"),\n",
    "        revision=config[\"revision\"],\n",
    "        torch_dtype=weight_dtype,\n",
    "        safety_checker=None,\n",
    "    )\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    lora_state_dict = get_module_kohya_state_dict(unet, \"lora_unet\", weight_dtype)\n",
    "    pipeline.load_lora_weights(lora_state_dict)\n",
    "    pipeline.fuse_lora()\n",
    "\n",
    "    pipeline = pipeline.to(accelerator.device, dtype=weight_dtype)\n",
    "    if config.get(\"enable_xformers_memory_efficient_attention\", False):\n",
    "        pipeline.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    if config.get(\"seed\", None) is None:\n",
    "        generator = None\n",
    "    else:\n",
    "        generator = torch.Generator(device=accelerator.device).manual_seed(config[\"seed\"])\n",
    "\n",
    "    validation_prompts = [\n",
    "        \"portrait photo of a girl, photograph, highly detailed face, depth of field, moody light, golden hour, style by Dan Winters, Russell James, Steve McCurry, centered, extremely detailed, Nikon D850, award winning photography\",\n",
    "        \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\",\n",
    "        \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n",
    "        \"A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece\",\n",
    "    ]\n",
    "\n",
    "    for _, prompt in enumerate(validation_prompts):\n",
    "        with autocast_ctx:\n",
    "            images = pipeline(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=4,\n",
    "                num_images_per_prompt=4,\n",
    "                generator=generator,\n",
    "                guidance_scale=1.0,\n",
    "            ).images\n",
    "\n",
    "            for _, image in enumerate(images):\n",
    "                image.save(f\"{config['output_dir']}/val_image_{step}_{_}.png\")\n",
    "\n",
    "    del pipeline\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From LatentConsistencyModel.get_guidance_scale_embedding\n",
    "def guidance_scale_embedding(w, embedding_dim=512, dtype=torch.float32):\n",
    "    assert len(w.shape) == 1\n",
    "    w = w * 1000.0\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)\n",
    "    emb = w.to(dtype)[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1))\n",
    "    assert emb.shape == (w.shape[0], embedding_dim)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def append_dims(x, target_dims):\n",
    "    dims_to_append = target_dims - x.ndim\n",
    "    if dims_to_append < 0:\n",
    "        raise ValueError(f\"input has {x.ndim} dims but target_dims is {target_dims}, which is less\")\n",
    "    return x[(...,) + (None,) * dims_to_append]\n",
    "\n",
    "\n",
    "# From LCMScheduler.get_scalings_for_boundary_condition_discrete\n",
    "def scalings_for_boundary_conditions(timestep, sigma_data=0.5, timestep_scaling=10.0):\n",
    "    scaled_timestep = timestep_scaling * timestep\n",
    "    c_skip = sigma_data**2 / (scaled_timestep**2 + sigma_data**2)\n",
    "    c_out = scaled_timestep / (scaled_timestep**2 + sigma_data**2) ** 0.5\n",
    "    return c_skip, c_out\n",
    "\n",
    "\n",
    "# Compare LCMScheduler.step, Step 4\n",
    "def get_predicted_original_sample(model_output, timesteps, sample, prediction_type, alphas, sigmas):\n",
    "    alphas = extract_into_tensor(alphas, timesteps, sample.shape)\n",
    "    sigmas = extract_into_tensor(sigmas, timesteps, sample.shape)\n",
    "    if prediction_type == \"epsilon\":\n",
    "        pred_x_0 = (sample - sigmas * model_output) / alphas\n",
    "    elif prediction_type == \"sample\":\n",
    "        pred_x_0 = model_output\n",
    "    elif prediction_type == \"v_prediction\":\n",
    "        pred_x_0 = alphas * sample - sigmas * model_output\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Prediction type {prediction_type} is not supported; currently, `epsilon`, `sample`, and `v_prediction`\"\n",
    "            f\" are supported.\"\n",
    "        )\n",
    "\n",
    "    return pred_x_0\n",
    "\n",
    "\n",
    "# Based on step 4 in DDIMScheduler.step\n",
    "def get_predicted_noise(model_output, timesteps, sample, prediction_type, alphas, sigmas):\n",
    "    alphas = extract_into_tensor(alphas, timesteps, sample.shape)\n",
    "    sigmas = extract_into_tensor(sigmas, timesteps, sample.shape)\n",
    "    if prediction_type == \"epsilon\":\n",
    "        pred_epsilon = model_output\n",
    "    elif prediction_type == \"sample\":\n",
    "        pred_epsilon = (sample - alphas * model_output) / sigmas\n",
    "    elif prediction_type == \"v_prediction\":\n",
    "        pred_epsilon = alphas * model_output + sigmas * sample\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Prediction type {prediction_type} is not supported; currently, `epsilon`, `sample`, and `v_prediction`\"\n",
    "            f\" are supported.\"\n",
    "        )\n",
    "\n",
    "    return pred_epsilon\n",
    "\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "class DDIMSolver:\n",
    "    def __init__(self, alpha_cumprods, timesteps=1000, ddim_timesteps=50):\n",
    "        step_ratio = timesteps // ddim_timesteps\n",
    "        self.ddim_timesteps = (np.arange(1, ddim_timesteps + 1) * step_ratio).round().astype(np.int64) - 1\n",
    "        self.ddim_alpha_cumprods = alpha_cumprods[self.ddim_timesteps]\n",
    "        self.ddim_alpha_cumprods_prev = np.asarray(\n",
    "            [alpha_cumprods[0]] + alpha_cumprods[self.ddim_timesteps[:-1]].tolist()\n",
    "        )\n",
    "        self.ddim_timesteps = torch.from_numpy(self.ddim_timesteps).long()\n",
    "        self.ddim_alpha_cumprods = torch.from_numpy(self.ddim_alpha_cumprods)\n",
    "        self.ddim_alpha_cumprods_prev = torch.from_numpy(self.ddim_alpha_cumprods_prev)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.ddim_timesteps = self.ddim_timesteps.to(device)\n",
    "        self.ddim_alpha_cumprods = self.ddim_alpha_cumprods.to(device)\n",
    "        self.ddim_alpha_cumprods_prev = self.ddim_alpha_cumprods_prev.to(device)\n",
    "        return self\n",
    "\n",
    "    def ddim_step(self, pred_x0, pred_noise, timestep_index):\n",
    "        alpha_cumprod_prev = extract_into_tensor(self.ddim_alpha_cumprods_prev, timestep_index, pred_x0.shape)\n",
    "        dir_xt = (1.0 - alpha_cumprod_prev).sqrt() * pred_noise\n",
    "        x_prev = alpha_cumprod_prev.sqrt() * pred_x0 + dir_xt\n",
    "        return x_prev\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(target_params, source_params, rate=0.99):\n",
    "    for targ, src in zip(target_params, source_params):\n",
    "        targ.detach().mul_(rate).add_(src, alpha=1 - rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model_class_from_model_name_or_path(\n",
    "    pretrained_model_name_or_path: str, revision: str, subfolder: str = \"text_encoder\"\n",
    "):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=subfolder, revision=revision\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "\n",
    "        return CLIPTextModel\n",
    "    elif model_class == \"CLIPTextModelWithProjection\":\n",
    "        from transformers import CLIPTextModelWithProjection\n",
    "\n",
    "        return CLIPTextModelWithProjection\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")\n",
    "\n",
    "\n",
    "# Adapted from pipelines.StableDiffusionPipeline.encode_prompt\n",
    "def encode_prompt(prompt_batch, text_encoder, tokenizer, proportion_empty_prompts, is_train=True):\n",
    "    captions = []\n",
    "    for caption in prompt_batch:\n",
    "        if random.random() < proportion_empty_prompts:\n",
    "            captions.append(\"\")\n",
    "        elif isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            captions.append(random.choice(caption) if is_train else caption[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_inputs = tokenizer(\n",
    "            captions,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(text_encoder.device))[0]\n",
    "\n",
    "    return prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/09/2024 18:58:39 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'dynamic_thresholding_ratio', 'variance_type', 'timestep_spacing', 'prediction_type', 'thresholding', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "/home/node/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'latents_mean', 'scaling_factor', 'force_upcast', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "{'num_class_embeds', 'class_embeddings_concat', 'mid_block_type', 'attention_type', 'conv_out_kernel', 'resnet_time_scale_shift', 'only_cross_attention', 'time_embedding_type', 'encoder_hid_dim_type', 'num_attention_heads', 'upcast_attention', 'transformer_layers_per_block', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'cross_attention_norm', 'resnet_out_scale_factor', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'dropout', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_embedding_dim', 'use_linear_projection', 'class_embed_type', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.\n",
      "{'num_class_embeds', 'class_embeddings_concat', 'mid_block_type', 'attention_type', 'conv_out_kernel', 'resnet_time_scale_shift', 'only_cross_attention', 'time_embedding_type', 'encoder_hid_dim_type', 'num_attention_heads', 'upcast_attention', 'transformer_layers_per_block', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'cross_attention_norm', 'resnet_out_scale_factor', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'dropout', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_embedding_dim', 'use_linear_projection', 'class_embed_type', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.\n",
      "06/09/2024 18:58:43 - INFO - __main__ - ***** Running training *****\n",
      "06/09/2024 18:58:43 - INFO - __main__ -   Num examples = 130000\n",
      "06/09/2024 18:58:43 - INFO - __main__ -   Num Epochs = 1\n",
      "06/09/2024 18:58:43 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "06/09/2024 18:58:43 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "06/09/2024 18:58:43 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "06/09/2024 18:58:43 - INFO - __main__ -   Total optimization steps = 1000\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/node/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      " 20%|██        | 200/1000 [08:05<32:04,  2.41s/it, loss=0.0326, lr=0.0001] 06/09/2024 19:06:48 - INFO - accelerate.accelerator - Saving current state to /home/node/academic/lcm/outputs/checkpoint-200\n",
      "06/09/2024 19:06:57 - INFO - accelerate.checkpointing - Model weights saved in /home/node/academic/lcm/outputs/checkpoint-200/model.safetensors\n",
      "06/09/2024 19:06:57 - INFO - accelerate.checkpointing - Optimizer state saved in /home/node/academic/lcm/outputs/checkpoint-200/optimizer.bin\n",
      "06/09/2024 19:06:57 - INFO - accelerate.checkpointing - Scheduler state saved in /home/node/academic/lcm/outputs/checkpoint-200/scheduler.bin\n",
      "06/09/2024 19:06:57 - INFO - accelerate.checkpointing - Gradient scaler state saved in /home/node/academic/lcm/outputs/checkpoint-200/scaler.pt\n",
      "06/09/2024 19:06:57 - INFO - accelerate.checkpointing - Random states saved in /home/node/academic/lcm/outputs/checkpoint-200/random_states_0.pkl\n",
      "06/09/2024 19:06:57 - INFO - __main__ - Saved state to /home/node/academic/lcm/outputs/checkpoint-200\n",
      "06/09/2024 19:06:57 - INFO - __main__ - Running validation...\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'timestep_scaling', 'dynamic_thresholding_ratio', 'timestep_spacing', 'original_inference_steps', 'prediction_type', 'thresholding', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "{'num_class_embeds', 'class_embeddings_concat', 'mid_block_type', 'attention_type', 'conv_out_kernel', 'resnet_time_scale_shift', 'only_cross_attention', 'time_embedding_type', 'encoder_hid_dim_type', 'num_attention_heads', 'upcast_attention', 'transformer_layers_per_block', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'cross_attention_norm', 'resnet_out_scale_factor', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'dropout', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_embedding_dim', 'use_linear_projection', 'class_embed_type', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:01<00:00,  3.05it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Kohya-style checkpoint detected.\n",
      "Loading unet.\n",
      " 40%|████      | 400/1000 [16:33<24:56,  2.49s/it, loss=0.00642, lr=0.0001] 06/09/2024 19:15:16 - INFO - accelerate.accelerator - Saving current state to /home/node/academic/lcm/outputs/checkpoint-400\n",
      "06/09/2024 19:15:20 - INFO - accelerate.checkpointing - Model weights saved in /home/node/academic/lcm/outputs/checkpoint-400/model.safetensors\n",
      "06/09/2024 19:15:20 - INFO - accelerate.checkpointing - Optimizer state saved in /home/node/academic/lcm/outputs/checkpoint-400/optimizer.bin\n",
      "06/09/2024 19:15:20 - INFO - accelerate.checkpointing - Scheduler state saved in /home/node/academic/lcm/outputs/checkpoint-400/scheduler.bin\n",
      "06/09/2024 19:15:20 - INFO - accelerate.checkpointing - Gradient scaler state saved in /home/node/academic/lcm/outputs/checkpoint-400/scaler.pt\n",
      "06/09/2024 19:15:20 - INFO - accelerate.checkpointing - Random states saved in /home/node/academic/lcm/outputs/checkpoint-400/random_states_0.pkl\n",
      "06/09/2024 19:15:20 - INFO - __main__ - Saved state to /home/node/academic/lcm/outputs/checkpoint-400\n",
      "06/09/2024 19:15:20 - INFO - __main__ - Running validation...\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'timestep_scaling', 'dynamic_thresholding_ratio', 'timestep_spacing', 'original_inference_steps', 'prediction_type', 'thresholding', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "{'num_class_embeds', 'class_embeddings_concat', 'mid_block_type', 'attention_type', 'conv_out_kernel', 'resnet_time_scale_shift', 'only_cross_attention', 'time_embedding_type', 'encoder_hid_dim_type', 'num_attention_heads', 'upcast_attention', 'transformer_layers_per_block', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'cross_attention_norm', 'resnet_out_scale_factor', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'dropout', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_embedding_dim', 'use_linear_projection', 'class_embed_type', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:02<00:00,  2.86it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Kohya-style checkpoint detected.\n",
      "Loading unet.\n",
      " 60%|██████    | 600/1000 [24:57<16:01,  2.40s/it, loss=0.0187, lr=0.0001]   06/09/2024 19:23:40 - INFO - accelerate.accelerator - Saving current state to /home/node/academic/lcm/outputs/checkpoint-600\n",
      "06/09/2024 19:23:47 - INFO - accelerate.checkpointing - Model weights saved in /home/node/academic/lcm/outputs/checkpoint-600/model.safetensors\n",
      "06/09/2024 19:23:47 - INFO - accelerate.checkpointing - Optimizer state saved in /home/node/academic/lcm/outputs/checkpoint-600/optimizer.bin\n",
      "06/09/2024 19:23:47 - INFO - accelerate.checkpointing - Scheduler state saved in /home/node/academic/lcm/outputs/checkpoint-600/scheduler.bin\n",
      "06/09/2024 19:23:47 - INFO - accelerate.checkpointing - Gradient scaler state saved in /home/node/academic/lcm/outputs/checkpoint-600/scaler.pt\n",
      "06/09/2024 19:23:47 - INFO - accelerate.checkpointing - Random states saved in /home/node/academic/lcm/outputs/checkpoint-600/random_states_0.pkl\n",
      "06/09/2024 19:23:47 - INFO - __main__ - Saved state to /home/node/academic/lcm/outputs/checkpoint-600\n",
      "06/09/2024 19:23:47 - INFO - __main__ - Running validation...\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'timestep_scaling', 'dynamic_thresholding_ratio', 'timestep_spacing', 'original_inference_steps', 'prediction_type', 'thresholding', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "{'num_class_embeds', 'class_embeddings_concat', 'mid_block_type', 'attention_type', 'conv_out_kernel', 'resnet_time_scale_shift', 'only_cross_attention', 'time_embedding_type', 'encoder_hid_dim_type', 'num_attention_heads', 'upcast_attention', 'transformer_layers_per_block', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'cross_attention_norm', 'resnet_out_scale_factor', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'dropout', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_embedding_dim', 'use_linear_projection', 'class_embed_type', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:02<00:00,  2.73it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Kohya-style checkpoint detected.\n",
      "Loading unet.\n",
      " 80%|████████  | 800/1000 [33:27<07:54,  2.37s/it, loss=0.0715, lr=0.0001] 06/09/2024 19:32:10 - INFO - accelerate.accelerator - Saving current state to /home/node/academic/lcm/outputs/checkpoint-800\n",
      "06/09/2024 19:32:13 - INFO - accelerate.checkpointing - Model weights saved in /home/node/academic/lcm/outputs/checkpoint-800/model.safetensors\n",
      "06/09/2024 19:32:13 - INFO - accelerate.checkpointing - Optimizer state saved in /home/node/academic/lcm/outputs/checkpoint-800/optimizer.bin\n",
      "06/09/2024 19:32:13 - INFO - accelerate.checkpointing - Scheduler state saved in /home/node/academic/lcm/outputs/checkpoint-800/scheduler.bin\n",
      "06/09/2024 19:32:13 - INFO - accelerate.checkpointing - Gradient scaler state saved in /home/node/academic/lcm/outputs/checkpoint-800/scaler.pt\n",
      "06/09/2024 19:32:13 - INFO - accelerate.checkpointing - Random states saved in /home/node/academic/lcm/outputs/checkpoint-800/random_states_0.pkl\n",
      "06/09/2024 19:32:13 - INFO - __main__ - Saved state to /home/node/academic/lcm/outputs/checkpoint-800\n",
      "06/09/2024 19:32:13 - INFO - __main__ - Running validation...\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'timestep_scaling', 'dynamic_thresholding_ratio', 'timestep_spacing', 'original_inference_steps', 'prediction_type', 'thresholding', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "{'num_class_embeds', 'class_embeddings_concat', 'mid_block_type', 'attention_type', 'conv_out_kernel', 'resnet_time_scale_shift', 'only_cross_attention', 'time_embedding_type', 'encoder_hid_dim_type', 'num_attention_heads', 'upcast_attention', 'transformer_layers_per_block', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'cross_attention_norm', 'resnet_out_scale_factor', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'dropout', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_embedding_dim', 'use_linear_projection', 'class_embed_type', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:02<00:00,  2.82it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Kohya-style checkpoint detected.\n",
      "Loading unet.\n",
      "100%|██████████| 1000/1000 [41:54<00:00,  2.63s/it, loss=0.128, lr=0.0001]  06/09/2024 19:40:37 - INFO - accelerate.accelerator - Saving current state to /home/node/academic/lcm/outputs/checkpoint-1000\n",
      "06/09/2024 19:40:48 - INFO - accelerate.checkpointing - Model weights saved in /home/node/academic/lcm/outputs/checkpoint-1000/model.safetensors\n",
      "06/09/2024 19:40:48 - INFO - accelerate.checkpointing - Optimizer state saved in /home/node/academic/lcm/outputs/checkpoint-1000/optimizer.bin\n",
      "06/09/2024 19:40:48 - INFO - accelerate.checkpointing - Scheduler state saved in /home/node/academic/lcm/outputs/checkpoint-1000/scheduler.bin\n",
      "06/09/2024 19:40:48 - INFO - accelerate.checkpointing - Gradient scaler state saved in /home/node/academic/lcm/outputs/checkpoint-1000/scaler.pt\n",
      "06/09/2024 19:40:48 - INFO - accelerate.checkpointing - Random states saved in /home/node/academic/lcm/outputs/checkpoint-1000/random_states_0.pkl\n",
      "06/09/2024 19:40:48 - INFO - __main__ - Saved state to /home/node/academic/lcm/outputs/checkpoint-1000\n",
      "06/09/2024 19:40:48 - INFO - __main__ - Running validation...\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'timestep_scaling', 'dynamic_thresholding_ratio', 'timestep_spacing', 'original_inference_steps', 'prediction_type', 'thresholding', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "{'num_class_embeds', 'class_embeddings_concat', 'mid_block_type', 'attention_type', 'conv_out_kernel', 'resnet_time_scale_shift', 'only_cross_attention', 'time_embedding_type', 'encoder_hid_dim_type', 'num_attention_heads', 'upcast_attention', 'transformer_layers_per_block', 'time_cond_proj_dim', 'mid_block_only_cross_attention', 'cross_attention_norm', 'resnet_out_scale_factor', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'dropout', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'time_embedding_act_fn', 'dual_cross_attention', 'time_embedding_dim', 'use_linear_projection', 'class_embed_type', 'addition_time_embed_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:01<00:00,  3.09it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Kohya-style checkpoint detected.\n",
      "Loading unet.\n",
      "Model weights saved in /home/node/academic/lcm/outputs/unet_lora/pytorch_lora_weights.safetensors\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up accelerator\n",
    "accelerator_project_config = ProjectConfiguration(project_dir=config[\"output_dir\"], logging_dir=os.path.join(config[\"output_dir\"], \"logs\"))\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config[\"gradient_accumulation_steps\"],\n",
    "    mixed_precision=config[\"mixed_precision\"],\n",
    "    log_with=\"tensorboard\",\n",
    "    project_config=accelerator_project_config,\n",
    ")\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# Set random seed\n",
    "if config[\"seed\"] is not None:\n",
    "    set_seed(config[\"seed\"])\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Load models and create solver\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(config[\"pretrained_teacher_model\"], subfolder=\"scheduler\")\n",
    "alpha_schedule = torch.sqrt(noise_scheduler.alphas_cumprod)\n",
    "sigma_schedule = torch.sqrt(1 - noise_scheduler.alphas_cumprod)\n",
    "solver = DDIMSolver(noise_scheduler.alphas_cumprod.numpy(), timesteps=noise_scheduler.config.num_train_timesteps, ddim_timesteps=50)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"pretrained_teacher_model\"], subfolder=\"tokenizer\", use_fast=False)\n",
    "text_encoder = CLIPTextModel.from_pretrained(config[\"pretrained_teacher_model\"], subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(config[\"pretrained_teacher_model\"], subfolder=\"vae\")\n",
    "teacher_unet = UNet2DConditionModel.from_pretrained(config[\"pretrained_teacher_model\"], subfolder=\"unet\")\n",
    "\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "teacher_unet.requires_grad_(False)\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(config[\"pretrained_teacher_model\"], subfolder=\"unet\")\n",
    "unet.train()\n",
    "\n",
    "if accelerator.unwrap_model(unet).dtype != torch.float32:\n",
    "    raise ValueError(f\"Student UNet loaded as datatype {accelerator.unwrap_model(unet).dtype}. Please make sure to load the model weights in full precision.\")\n",
    "\n",
    "# Add LoRA to the student UNet\n",
    "lora_config = LoraConfig(r=config[\"lora_rank\"], target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"proj_in\", \"proj_out\", \"ff.net.0.proj\", \"ff.net.2\", \"conv1\", \"conv2\", \"conv_shortcut\", \"downsamplers.0.conv\", \"upsamplers.0.conv\", \"time_emb_proj\"], lora_alpha=config[\"lora_rank\"], lora_dropout=0.0)\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "\n",
    "# Handle mixed precision and device placement\n",
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "vae.to(accelerator.device)\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "teacher_unet.to(accelerator.device)\n",
    "\n",
    "alpha_schedule = alpha_schedule.to(accelerator.device)\n",
    "sigma_schedule = sigma_schedule.to(accelerator.device)\n",
    "solver = solver.to(accelerator.device)\n",
    "\n",
    "# Create optimizer\n",
    "if config[\"use_8bit_adam\"]:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    except ImportError:\n",
    "        raise ImportError(\"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\")\n",
    "else:\n",
    "    optimizer_class = torch.optim.AdamW\n",
    "\n",
    "optimizer = optimizer_class(\n",
    "    unet.parameters(),\n",
    "    lr=config[\"learning_rate\"],\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=config[\"adam_weight_decay\"],\n",
    "    eps=1e-08,\n",
    ")\n",
    "def compute_embeddings(prompt_batch, proportion_empty_prompts, text_encoder, tokenizer, is_train=True):\n",
    "    prompt_embeds = encode_prompt(prompt_batch, text_encoder, tokenizer, proportion_empty_prompts, is_train)\n",
    "    return {\"prompt_embeds\": prompt_embeds}\n",
    "# Create dataset and dataloader\n",
    "dataset = SDText2ImageDataset(\n",
    "    train_shards_path_or_url=config[\"train_shards_path_or_url\"],\n",
    "    num_train_examples=config[\"max_train_samples\"],\n",
    "    per_gpu_batch_size=config[\"train_batch_size\"],\n",
    "    global_batch_size=config[\"train_batch_size\"] * accelerator.num_processes,\n",
    "    num_workers=config[\"dataloader_num_workers\"],\n",
    "    resolution=config[\"resolution\"],\n",
    "    interpolation_type=\"bilinear\",\n",
    "    shuffle_buffer_size=1000,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "train_dataloader = dataset.train_dataloader\n",
    "\n",
    "compute_embeddings_fn = functools.partial(\n",
    "compute_embeddings,\n",
    "proportion_empty_prompts=0,\n",
    "text_encoder=text_encoder,\n",
    "tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(train_dataloader.num_batches / config[\"gradient_accumulation_steps\"])\n",
    "if config[\"max_train_steps\"] is None:\n",
    "    config[\"max_train_steps\"] = config[\"num_train_epochs\"] * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    config[\"lr_scheduler\"],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.get(\"lr_warmup_steps\", 0),\n",
    "    num_training_steps=config[\"max_train_steps\"],\n",
    ")\n",
    "\n",
    "# Prepare everything with accelerator\n",
    "unet, optimizer, lr_scheduler = accelerator.prepare(unet, optimizer, lr_scheduler)\n",
    "\n",
    "# Recalculate total training steps\n",
    "num_update_steps_per_epoch = math.ceil(train_dataloader.num_batches / config[\"gradient_accumulation_steps\"])\n",
    "if overrode_max_train_steps:\n",
    "    config[\"max_train_steps\"] = config[\"num_train_epochs\"] * num_update_steps_per_epoch\n",
    "config[\"num_train_epochs\"] = math.ceil(config[\"max_train_steps\"] / num_update_steps_per_epoch)\n",
    "\n",
    "# Initialize trackers\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.init_trackers(\"lcm-lora\", config=config)\n",
    "\n",
    "uncond_input_ids = tokenizer([\"\"] * config[\"train_batch_size\"], return_tensors=\"pt\", padding=\"max_length\", max_length=77).input_ids.to(accelerator.device)\n",
    "uncond_prompt_embeds = text_encoder(uncond_input_ids)[0]\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    autocast_ctx = nullcontext()\n",
    "else:\n",
    "    autocast_ctx = torch.autocast(accelerator.device.type)\n",
    "\n",
    "# Train!\n",
    "total_batch_size = config[\"train_batch_size\"] * accelerator.num_processes * config[\"gradient_accumulation_steps\"]\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {train_dataloader.num_samples}\")\n",
    "logger.info(f\"  Num Epochs = {config['num_train_epochs']}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {config['train_batch_size']}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {config['gradient_accumulation_steps']}\")\n",
    "logger.info(f\"  Total optimization steps = {config['max_train_steps']}\")\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "progress_bar = tqdm(range(global_step, config[\"max_train_steps\"]), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "for epoch in range(first_epoch, config[\"num_train_epochs\"]):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Load and process the image and text conditioning\n",
    "            image, text = batch\n",
    "            image = image.to(accelerator.device, non_blocking=True)\n",
    "            encoded_text = compute_embeddings_fn(text)\n",
    "\n",
    "            pixel_values = image.to(dtype=weight_dtype)\n",
    "            if vae.dtype != weight_dtype:\n",
    "                vae.to(dtype=weight_dtype)\n",
    "\n",
    "            # Encode pixel values with batch size of at most config[\"vae_encode_batch_size\"]\n",
    "            latents = []\n",
    "            for i in range(0, pixel_values.shape[0], config[\"vae_encode_batch_size\"]):\n",
    "                latents.append(vae.encode(pixel_values[i : i + config[\"vae_encode_batch_size\"]]).latent_dist.sample())\n",
    "            latents = torch.cat(latents, dim=0)\n",
    "\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "            latents = latents.to(weight_dtype)\n",
    "            bsz = latents.shape[0]\n",
    "\n",
    "            # Sample random timestep and starting timestep for each image\n",
    "            topk = noise_scheduler.config.num_train_timesteps // 50\n",
    "            index = torch.randint(0, 50, (bsz,), device=latents.device).long()\n",
    "            start_timesteps = solver.ddim_timesteps[index]\n",
    "            timesteps = start_timesteps - topk\n",
    "            timesteps = torch.where(timesteps < 0, torch.zeros_like(timesteps), timesteps)\n",
    "\n",
    "            # Get boundary scalings for start and end timesteps\n",
    "            c_skip_start, c_out_start = scalings_for_boundary_conditions(start_timesteps, timestep_scaling=10.0)\n",
    "            c_skip_start, c_out_start = [append_dims(x, latents.ndim) for x in [c_skip_start, c_out_start]]\n",
    "            c_skip, c_out = scalings_for_boundary_conditions(timesteps, timestep_scaling=10.0)\n",
    "            c_skip, c_out = [append_dims(x, latents.ndim) for x in [c_skip, c_out]]\n",
    "\n",
    "            # Sample noise and add it to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            noisy_model_input = noise_scheduler.add_noise(latents, noise, start_timesteps)\n",
    "\n",
    "            # Sample random guidance scale\n",
    "            w = (config[\"w_max\"] - config[\"w_min\"]) * torch.rand((bsz,)) + config[\"w_min\"]\n",
    "            w = w.reshape(bsz, 1, 1, 1).to(latents.device, dtype=latents.dtype)\n",
    "\n",
    "            prompt_embeds = encoded_text[\"prompt_embeds\"]\n",
    "            noise_pred = unet(noisy_model_input, start_timesteps, timestep_cond=None, encoder_hidden_states=prompt_embeds.float(), added_cond_kwargs=encoded_text).sample\n",
    "\n",
    "            pred_x_0 = get_predicted_original_sample(noise_pred, start_timesteps, noisy_model_input, noise_scheduler.config.prediction_type, alpha_schedule, sigma_schedule)\n",
    "            model_pred = c_skip_start * noisy_model_input + c_out_start * pred_x_0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with autocast_ctx:\n",
    "                    # Get teacher model predictions\n",
    "                    cond_teacher_output = teacher_unet(noisy_model_input.to(weight_dtype), start_timesteps, encoder_hidden_states=prompt_embeds.to(weight_dtype)).sample\n",
    "                    cond_pred_x0 = get_predicted_original_sample(cond_teacher_output, start_timesteps, noisy_model_input, noise_scheduler.config.prediction_type, alpha_schedule, sigma_schedule)\n",
    "                    cond_pred_noise = get_predicted_noise(cond_teacher_output, start_timesteps, noisy_model_input, noise_scheduler.config.prediction_type, alpha_schedule, sigma_schedule)\n",
    "\n",
    "                    uncond_teacher_output = teacher_unet(noisy_model_input.to(weight_dtype), start_timesteps, encoder_hidden_states=uncond_prompt_embeds.to(weight_dtype)).sample\n",
    "                    uncond_pred_x0 = get_predicted_original_sample(uncond_teacher_output, start_timesteps, noisy_model_input, noise_scheduler.config.prediction_type, alpha_schedule, sigma_schedule)\n",
    "                    uncond_pred_noise = get_predicted_noise(uncond_teacher_output, start_timesteps, noisy_model_input, noise_scheduler.config.prediction_type, alpha_schedule, sigma_schedule)\n",
    "\n",
    "                    # Calculate CFG estimates\n",
    "                    pred_x0 = cond_pred_x0 + w * (cond_pred_x0 - uncond_pred_x0)\n",
    "                    pred_noise = cond_pred_noise + w * (cond_pred_noise - uncond_pred_noise)\n",
    "\n",
    "                    # Run one step of ODE solver\n",
    "                    x_prev = solver.ddim_step(pred_x0, pred_noise, index)\n",
    "\n",
    "            # Get target LCM prediction\n",
    "            with torch.no_grad():\n",
    "                with autocast_ctx:\n",
    "                    target_noise_pred = unet(x_prev.float(), timesteps, timestep_cond=None, encoder_hidden_states=prompt_embeds.float()).sample\n",
    "                pred_x_0 = get_predicted_original_sample(target_noise_pred, timesteps, x_prev, noise_scheduler.config.prediction_type, alpha_schedule, sigma_schedule)\n",
    "                target = c_skip * x_prev + c_out * pred_x_0\n",
    "\n",
    "            # Calculate loss\n",
    "            if config[\"loss_type\"] == \"l2\":\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "            elif config[\"loss_type\"] == \"huber\":\n",
    "                loss = torch.mean(torch.sqrt((model_pred.float() - target.float()) ** 2 + 0.001**2) - 0.001)\n",
    "\n",
    "            # Backpropagate and update weights\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            # Log training progress\n",
    "            if accelerator.is_main_process:\n",
    "                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "                progress_bar.set_postfix(**logs)\n",
    "                accelerator.log(logs, step=global_step)\n",
    "\n",
    "                # Save model checkpoints\n",
    "                if global_step % config[\"checkpointing_steps\"] == 0:\n",
    "                    save_path = os.path.join(config[\"output_dir\"], f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "                    logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "                # Run validation\n",
    "                if global_step % config[\"validation_steps\"] == 0:\n",
    "                    log_validation(vae, unet, config, accelerator, weight_dtype, global_step)\n",
    "\n",
    "        if global_step >= config[\"max_train_steps\"]:\n",
    "            break\n",
    "\n",
    "# Save the final model\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    unet = accelerator.unwrap_model(unet)\n",
    "    unet.save_pretrained(config[\"output_dir\"])\n",
    "    lora_state_dict = get_peft_model_state_dict(unet, adapter_name=\"default\")\n",
    "    StableDiffusionPipeline.save_lora_weights(os.path.join(config[\"output_dir\"], \"unet_lora\"), lora_state_dict)\n",
    "\n",
    "accelerator.end_training()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
